{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f288f3a7-35f7-4d85-a402-7b61cb274fc5",
   "metadata": {},
   "source": [
    "# Chatwolf\n",
    "\n",
    "Das Projekt \"Chatwolf\" versucht die M√∂glichkeiten von Services, die generative k√ºnstliche Intelligenz (KI) einer breiten √ñffentlichkeit zur Verf√ºgung stellt, in den Prozess der quantitativen Inhaltsanalyse einzubinden, und niederschwellig und kosteng√ºnstig nutzbar zu machen.\n",
    "\n",
    "![Chatwolf Bild](./chatwolf_round_small.png)\n",
    "\n",
    "1. [Hintergrund](#Hintergrund)\n",
    "2. [Workflow](#Workflow)\n",
    "3. [Umsetzung I](#Umsetzung-der-Schritte-1-bis-5)\n",
    "4. [Umsetzung II](#Umsetzung-der-Schritte-7-bis-9)\n",
    "5. [Umsetzung III](#Umsetzung-der-Schritte-10-und-11)\n",
    "6. [Abschlussbemerkungen](#Abschlussbemerkungen)\n",
    "7. [Literatur](#Literatur)\n",
    "\n",
    "\n",
    "## Hintergrund\n",
    "In einem laufenden Projekt im Arbeitsbereich P√§dagogische Psychologie des Instituts f√ºr Psychologie der Universit√§t Graz, im Rahmen dessen mehrere Masterarbeiten betreut werden, werden verschiedene Aspekte von Interaktionen zwischen Tutor:innen und Lernenden betrachtet. Dabei werden drei Formen der Interaktion miteinander verglichen: 1) maschinelle:r Tutor:in schreibt (chatGPT-4 mit angepassten Instruktionen), 2) menschliche:r Tutor:in schreibt, 3) menschliche:r Tutor:in spricht. Bei den Lernenden handelt es sich dabei immer um Proband:innen. Mit den Mitteln quantitativer Inhaltsanalyse wird u.a. der Grad der sozialen Pr√§senz zwischen diesen verschiedenen Bedingungen verglichen. Unter sozialer Pr√§senz versteht man das Ausma√ü, in dem ein:e Kommunikationspartner:in in der medienvermittelten Kommunikation als reale Person wahrgenommen wird (Gunawardena, 1995).\n",
    "\n",
    "Inhaltsanalyse gr√∂√üerer Datenmengen ist aufw√§ndig und nimmt viel Zeit in Anspruch. Zur Unterst√ºtzung existieren verschiedene Softwarel√∂sungen, die sich in Grad der Unterst√ºtzung, Bedienungskomfort und v.a. Kosten stark unterscheiden. Generative KI, im speziellen Large Language Modelle (LLMs), eignen sich gut f√ºr die Verarbeitung und Zusammenfassung von Text (T√∂rneberg, 2023). Der Einsatz von generativer KI f√ºr die Inhaltsanalyse im Bereich psychologischer und soziologischer Forschung wurde auch bereits in kleinem Rahmen getestet und dokumentiert (z.B. Morgan, 2023). Erste eigene Erfahrungen haben gezeigt, dass die Entwicklung von Instruktionen f√ºr die Inhaltsanalyse mit chatGPT-4 zwar schnell scheinbar brauchbare Ergebnisse liefert, diese aber nicht zuverl√§ssig reproduzierbar sind. Insbesondere wird Textmaterial bei mehrmaligem Durchlauf unter Verwendung identischer Instruktionen in unterschiedliche Kodiereinheiten zerlegt. Es ist zwar davon auszugehen, dass sich das mit kommenden Versionen der entsprechenden LLMs verbessern wird. Im Moment ist diese Herangehensweise jedoch nicht f√ºr einen gro√üteils automatischen Ablauf geeignet.\n",
    "\n",
    "Daher wurde ein Workflow entwickelt, mit dem √ºberpr√ºft werden soll, in wie weit aktuelle LLMs zur Unterst√ºtzung der Inhaltsanalyse eingesetzt werden k√∂nnen.\n",
    "\n",
    "## Workflow\n",
    "1. Input: Chat-Protokolle (DOCX)\n",
    "2. Konvertierung ins Markdown-Format\n",
    "3. Aufteilen in Anteil Tutor:in und Lernende:r\n",
    "4. Syntax-Analyse: Aufteilen in Kodiereinheiten\n",
    "5. Output: Kodiereinheiten (Markdown-Datei)\n",
    "6. Manuelle √úberpr√ºfung der Output-Dateien\n",
    "7. Input: √úbergabe der Kodiereinheiten an chatGPT\n",
    "8. Kodierung durch chatGPT\n",
    "9. Output: Ergebnisse der Kodierung (CSV)\n",
    "10. Mehrmalige Wiederholung der Kodierung (7 bis 9)\n",
    "11. Analyse der Reliabilit√§t (√úbereinstimmungsma√üe)\n",
    "12. Analyse der Validit√§t anhand von menschlichen Kodierer:innen\n",
    "\n",
    "Die Schritte 1 bis 5 werden in Form eines Python-Programmes umgesetzt. Bei dem verwendeten Textmaterial handelt es sich um Chat-Protokolle von Dialogen zwischen Tutor:innen und Lernenden. Es kommen u.a. Python-Module zum Konvertieren zwischen Dateiformaten (Schritt 2) und zum Verarbeiten nat√ºrlicher Sprache (Schritt 4) zum Einsatz. Die daf√ºr verwendeten Module sind `pypandoc`und `spaCy`.\n",
    "\n",
    "In Schritt 6 erfolgt die manuelle √úberpr√ºfung und evtl. Anpassung der zuvor generierten Dateien.\n",
    "\n",
    "Die Schritte 7 bis 9 finden mit Unterst√ºtzung von chatGPT statt. In Schritt 7 wird die Liste der Kodiereinheiten (Tutor:in und Lernende:r getrennt) in Form einer zuvor generierten strukturierten Textdatei im Markdown-Format an chatGPT √ºbergeben. In Schritt 8 werden die Kodiereinheiten im Hinblick auf soziale Pr√§senz beurteilt. Dazu wird ein Kodierschema f√ºr menschliche Kodierer:innen entwickelt, auf dessen Basis wiederum detaillierte Instruktionen f√ºr chatGPT entwickelt werden. Die Instruktionen f√ºr chatGPT folgen dabei den Prinzipien von Structured Prompting (Mollick, 2023; Brand, 2023) und Chain-of-Thought Prompting (Wei et al., 2023). In Schritt 9 werden die  Ergebnisse der Kodierung in Form einer Textdatei im CSV-Format gesichert.\n",
    "\n",
    "Anschlie√üend (Schritt 10) werden die Schritte 7 bis 9 mehrmals wiederholt. Dabei ist es wichtig, dass diese Wiederholungen jeweils in einer neuen Instanz von chatGPT stattfinden, damit vorangegangene Eingaben und Ergebnisse, die sich noch im Kontext-Fenster befinden, die neuen Ergebnisse nicht kontaminieren.\n",
    "\n",
    "Schritt 11 wird in Form eines Python-Programmes umgesetzt. Dabei werden die Ergebnisse der mehrfach durchgef√ºhrten Kodierungsprozesse an den selben Ausgangsdaten auf √úbereinstimmung gepr√ºft. Da hier in jedem Datensatz alle Kodiereinheiten kodiert werden, kann ein Signifikanztest durchgef√ºhrt werden, der auf eine Abweichung von Null (= zuf√§llige √úbereinstimmung) pr√ºft. Damit kann eine statistische Aussage getroffen werden, wie zuverl√§ssig die wiederholten Kodierungen von chatGPT unter Anwendung der entwickelten Instruktionen sind.\n",
    "\n",
    "In Schritt 12 wird die Qualit√§t der f√ºr Schritt 8 entwickelten Instruktionen evaluiert, indem das Ergebnis der Kodierung durch chatGPT unter Anwendung der entwickelten Instruktionen mit dem Ergebnis der Kodierung durch menschliche Kodierer:innen unter Anwendung des entwickelten Kodierungsschemas verglichen wird (Interraterreliabilit√§t; Bortz & D√∂ring, 2006). Daf√ºr kann z.B. das Python-Modul `scipy.stats.kendalltau` eingesetzt werden. Bei bedeutsamen Abweichungen m√ºssen Kodieranweisungen und Instruktionen √ºberarbeitet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88297c-a5c2-41f8-92cb-cb60e73cc132",
   "metadata": {},
   "source": [
    "## Umsetzung der Schritte 1 bis 5<a name=\"umsetzung_i\" />\n",
    "### 0. Vorbereitungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af527c-015f-442c-bb55-452357ee27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies installieren\n",
    "#!pip install pypandoc spacy pyperclip pingouin\n",
    "#!spacy download de_core_news_md\n",
    "\n",
    "import os, re, pypandoc, spacy, pyperclip\n",
    "import pandas as pd\n",
    "\n",
    "demo = True    # Demo-Durchlauf\n",
    "demo_file = 0  # Welche Datei soll im Demo-Durchlauf verwendet werden?\n",
    "\n",
    "# Soll spacy dependency parse oder statistical sentence segmenter verwenden?\n",
    "# Dependency parse ist besser bei Texten wie Nachrichten, Zeitungsartikel etc.\n",
    "# Statistical sentence segmenter ist besser bei unklarer Punktuation.\n",
    "# nlp.enable_pipe wird beim import von spacy auf \"None\" gesetzt.\n",
    "segmenter = True\n",
    "if segmenter: \n",
    "  nlp = spacy.load('de_core_news_md', exclude=[\"parser\"])\n",
    "  nlp.enable_pipe(\"senter\")\n",
    "else:\n",
    "  nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e524fc-213f-4592-aa4f-daa2e418b2b2",
   "metadata": {},
   "source": [
    "### 1. Input: Dateien im DOCX-Format (Chat-Protokolle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a011e-cadf-4af8-8151-f37e0ded6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docx_filenames():\n",
    "  cwd = os.getcwd()\n",
    "  file_names = os.listdir(cwd)\n",
    "  docx_files = []\n",
    "  for file_name in file_names:\n",
    "    if file_name.endswith('.docx'):\n",
    "      docx_path = os.path.join(cwd, file_name)\n",
    "      docx_files.append(docx_path)\n",
    "  return docx_files\n",
    "\n",
    "if (demo):\n",
    "  docx_files = get_docx_filenames()\n",
    "  print(docx_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12277464-c713-43ab-9dfe-9c276b99977d",
   "metadata": {},
   "source": [
    "### 2. Konvertierung ins Markdown-Format (und Textbereinigung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07863218-6230-4b1e-bbd2-e593d468afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_docx_to_md(docx_path):\n",
    "  md_path = re.sub(r'\\.docx$', '.md', docx_path)\n",
    "  md_text = pypandoc.convert_file(docx_path, 'md', extra_args=['--wrap=none'])\n",
    "  # Entfernen der User-Icons\n",
    "  md_text = re.sub(r'\\!\\[User\\]\\(media.+', '', md_text)\n",
    "  # Bezeichnungen f√ºr User:in und Tutor:in anpassen\n",
    "  # Unter Windows wird das Zeilenende mit $ nicht korrekt erkannt,\n",
    "  # daher muss $ unter Windows in den folgenden RegEx weg gelassen werden.\n",
    "  #md_text = re.sub(r'^\\*\\*Sie\\*\\*$', '# User', md_text, flags=re.MULTILINE)\n",
    "  #md_text = re.sub(r'^\\*\\*You\\*\\*$', '# User', md_text, flags=re.MULTILINE)\n",
    "  #md_text = re.sub(r'^\\*\\*LernenGPT\\*\\*$', '# Tutor', md_text, flags=re.MULTILINE)\n",
    "  #md_text = re.sub(r'^\\\\\\[\\d\\d\\:\\d\\d\\\\\\] Testung Paedpsy$', '# User', md_text, flags=re.MULTILINE)\n",
    "  #md_text = re.sub(r'^\\\\\\[\\d\\d\\:\\d\\d\\\\\\]\\s\\w+.*,\\s\\w+(.\\(Extern\\))?$', '# Tutor', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\*\\*Sie\\*\\*', '# User', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\*\\*You\\*\\*', '# User', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\*\\*LernenGPT\\*\\*', '# Tutor', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\\\\\[\\d\\d\\:\\d\\d\\\\\\] Testung Paedpsy', '# User', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\\\\\[\\d\\d\\:\\d\\d\\\\\\]\\s\\w+.*,\\s\\w+(.\\(Extern\\))?', '# Tutor', md_text, flags=re.MULTILINE)\n",
    "  # Entfernen von Emote-Grafiken und anderen Grafiken: ![üòÑ](media/image7.png){width=\"0...in\" height=\"0...in\"}\n",
    "  md_text = re.sub(r'!\\[(.)\\]\\(media.+png\\)\\{width.+height.+in\\\"\\}', r'\\1', md_text)\n",
    "  md_text = re.sub(r'!\\[\\]\\(media.+[(png)(jpeg)(wmf)]\\)(\\{width.+height.+in\\\"\\})?', '', md_text)\n",
    "  # Ersetzen gesch√ºtzter Leerzeichen: \\xa0\n",
    "  md_text = md_text.replace(u'\\xa0', u' ')\n",
    "  # Gendern mit : macht evtl Probleme bei Erkennung von Satzgrenzen.\n",
    "  #md_text = re.sub(r'(\\w):in', r'\\1in', md_text)\n",
    "  return md_text\n",
    "\n",
    "if (demo):\n",
    "  md_text = convert_docx_to_md(docx_files[demo_file])\n",
    "  print(md_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc711fb-bf58-4cd9-be54-25fa138f1616",
   "metadata": {},
   "source": [
    "### 3. Aufteilen der Chat-Protokolle in Tutor:in und Lernende:r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d7071-73e9-4891-afe5-08b09a3d1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_speakers(md_text):\n",
    "  tmp = []\n",
    "  first_speaker = \"\"\n",
    "  current_speaker = \"\"\n",
    "  speakers = {\n",
    "    \"User\": [],\n",
    "    \"Tutor\": []\n",
    "  }\n",
    "\n",
    "  # Erste:n Sprecher:in festhalten\n",
    "  for line in md_text.splitlines():\n",
    "    if line == \"# User\":\n",
    "      first_speaker = \"User\"\n",
    "      break\n",
    "    elif line == \"# Tutor\":\n",
    "      first_speaker = \"Tutor\"\n",
    "      break\n",
    "  # Aufteilen der Datei in Anteile von User und Tutor\n",
    "  for line in md_text.splitlines():\n",
    "    if line == \"# User\":\n",
    "      current_speaker = \"User\"\n",
    "    elif line == \"# Tutor\":\n",
    "      current_speaker = \"Tutor\"\n",
    "    elif (line and current_speaker in speakers):\n",
    "      speakers[current_speaker].append(line)\n",
    "  return speakers, first_speaker\n",
    "\n",
    "if (demo):\n",
    "  speakers, first_speaker = split_speakers(md_text)\n",
    "  print(first_speaker)\n",
    "  print(speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48522c2b-faa7-46ae-91a3-74992b0b73fb",
   "metadata": {},
   "source": [
    "### 4. Syntax-Analyse: Aufteilen in Kodiereinheiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130dfdc5-109b-4db3-ad7d-fe6015530e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_props(speakers):\n",
    "  props = {\n",
    "    \"User\": [],\n",
    "    \"Tutor\": []\n",
    "  }\n",
    "  for actor in [\"User\", \"Tutor\"]:\n",
    "    for x in speakers[actor]:\n",
    "      doc = nlp(x)\n",
    "      for sentence in doc.sents:\n",
    "        props[actor].append(sentence.text.strip())\n",
    "  return props\n",
    "\n",
    "if (demo):\n",
    "  props = split_props(speakers)\n",
    "  print(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01dcf5-838f-4956-9b96-878160d74d3b",
   "metadata": {},
   "source": [
    "### 5. Output: Textdatei (Markdown) mit Kodiereinheiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d79ef-f5a5-4672-b1d2-bfc6c6bd8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text(md_text, props, path):\n",
    "  path = os.path.splitext(path)[0]\n",
    "  # Gesamte Konversation speichern\n",
    "  md_path = path + '.md'\n",
    "  with open(md_path, 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(md_text)\n",
    "  # User- und Tutor-Seite der Konversation speichern\n",
    "  for actor in [\"User\", \"Tutor\"]:\n",
    "    props_path = path + '_' + actor + '.md'\n",
    "    with open(props_path, 'w', encoding=\"utf-8\") as file:\n",
    "      for x in props[actor]:\n",
    "        file.write(x)\n",
    "        file.write(\"\\n\")\n",
    "  print(f'Processed {path}.docx')\n",
    "\n",
    "if demo:\n",
    "  save_text(md_text, props, docx_files[demo_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954e2cf-3482-4fd2-be20-8adaf2e062a1",
   "metadata": {},
   "source": [
    "### Optional: Gesamtdurchlauf der Schritte 1 bis 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7af442-77a4-47cf-9d37-b2b6d3a46432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesamtdurchlauf der Schritte 1 bis 5\n",
    "docx_files = get_docx_filenames()\n",
    "for file_name in docx_files:\n",
    "  md_text = convert_docx_to_md(file_name)\n",
    "  speakers, first_speaker = split_speakers(md_text)\n",
    "  props = split_props(speakers)\n",
    "  save_text(md_text, props, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f890b2-e330-4109-878b-0282e897c2d6",
   "metadata": {},
   "source": [
    "### Optional: Verzeichnis aufr√§umen\n",
    "**CAVE: Alle Dateien im aktuellen Verzeichnis mit der Endung `.md` werden gel√∂scht!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a42ee-6e02-415b-bbba-1e6e246aa0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnis aufr√§umen\n",
    "# ACHTUNG: Alle Dateien mit Endung .md werden gel√∂scht!\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "file_names = os.listdir(cwd)\n",
    "for file_name in file_names:\n",
    "  if file_name.endswith('.md'):\n",
    "    os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6004b3-f1eb-41fc-bf78-2cbd6146c7ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Manuelle √úberpr√ºfung der Output-Dateien\n",
    "Die generierten Output-Dateien m√ºssen manuell √ºberpr√ºft und evtl. korrigiert werden. Die deutschsprachigen Trainingsdaten f√ºr das verwendete Modul `spacy` wurden v.a. anhand von deutschspachigen Zeitungsartikeln gewonnen. Die Chats entsprechen aber bei weitem nicht dieser Qualit√§t, weshalb manuell nachgebessert werden muss.\n",
    "\n",
    "Um ein √úberschreiben der korrigierten Dateien zu vermeiden, wird empfohlen, diese unter neuem Namen oder in einem neuen Verzeichnis zu sichern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17b483-2baf-4f46-a4d1-d703f85dfaec",
   "metadata": {},
   "source": [
    "## Umsetzung der Schritte 7 bis 9\n",
    "*Die Schritte 7 bis 9 werden in ChatGPT durchgef√ºhrt.*\n",
    "\n",
    "### 7. Input: √úbergabe der Liste von Kodiereinheiten an chatGPT\n",
    "Dazu wurde ein Custom GPT erstellt, das unter dem folgenden Link zu erreichen ist: **[Custom GPT: Content-Analysis](https://chatgpt.com/g/g-tURb8JB2t-content-analysis)**\n",
    "\n",
    "Die Instruktionen f√ºr dieses Custom GPT sind im Anschluss angef√ºhrt und k√∂nnen auch als Prompt ausgef√ºhrt werden.\n",
    "\n",
    "```\n",
    "# Custom GPT - Instructions\n",
    "\n",
    "## Task\n",
    "Process the uploaded markdown file and evaluate each line for social presence, then present the results in a table.\n",
    "\n",
    "## Role: Text Analyst\n",
    "- Analyze text based on social presence indicators.\n",
    "- Evaluate the emotional and interpersonal engagement in the text.\n",
    "- Apply consistent scoring throughout the analysis.\n",
    "\n",
    "## Audience: Scientist\n",
    "- Wants to assess social presence in a document.\n",
    "- Prefers clear and structured output.\n",
    "- Expects accurate and reliable analysis.\n",
    "- Their research and career depend on the results!\n",
    "\n",
    "## Create\n",
    "A table with two columns: 1) Social Presence Rating and 2) Unit of analysis (as per input).\n",
    "\n",
    "### Create Rules\n",
    "- Take a deep breath before you begin!\n",
    "- Think step by step!\n",
    "- Rate each line of the input material either 0 (low social presence) or 1 (high social presence).\n",
    "- Create a markdown table with two columns: \"Rating\" and \"Unit\".\n",
    "\n",
    "### Create Example\n",
    "  | Rating | Unit |\n",
    "  | --- | --- |\n",
    "  | 0 | Vielleicht mit Beistrich? |\n",
    "  | 1 | Danke dir |\n",
    "  | 0 | zeig es mir! | 0 |\n",
    "  | 1 | üòÑ |\n",
    "  | 1 | like 1 |\n",
    "\n",
    "## Intent\n",
    "Help the user assess social presence in the text by providing unit-by-unit ratings.\n",
    "\n",
    "### Intent Rules\n",
    "- Focus on interpersonal connection and emotional tone.\n",
    "- Deliver the result in an easy-to-use format.\n",
    "- Make sure the results are reliable and can be reproduced.\n",
    "\n",
    "## Material\n",
    "- Textfile in markdown format.\n",
    "- Each line of the input file represents a single unit of analysis.\n",
    "```\n",
    "\n",
    "In ChatGPT kann nun eine der neu erstellten Dateien (XXX_User.md) hochgeladen werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ad096-fccb-4261-a83f-eb14812dcf88",
   "metadata": {},
   "source": [
    "### 8. Kodierung in Bezug auf soziale Pr√§senz\n",
    "\n",
    "F√ºr die Kodierung kann das folgende Prompt ausgef√ºhrt werden:\n",
    "\n",
    "`Rate the social presence for each unit of analysis in the uploaded file according to your instructions.`\n",
    "\n",
    "Der Text wird beim Ausf√ºhren des n√§chsten Code-Blocks in die Zwischenablage kopiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6efbd-6acd-438d-a786-3e1d371dcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyperclip.copy(\"Rate the social presence for each unit of analysis in the uploaded file according to your instructions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29482dff-2da9-4b48-ae8d-49fcf1163c5d",
   "metadata": {},
   "source": [
    "### 9. Output: Textdatei (CSV) mit den Resultaten der Kodierung\n",
    "Die Ergebnisse werden von ChatGPT in Form einer CSV-Datei zum Download bereit gestellt. Eventuell muss die Erstellung der CSV-Datei explizit angefordert werden.\n",
    "\n",
    "Allerdings hat chatGPT immer wieder versucht, die CSV-Dateien auf unterschiedliche Art und Weise zu erstellen. Manchmal mit Erfolg, √∂fter leider ohne, da ben√∂tigte Python-Module nicht installiert waren, oder das generierte Skript zu Fehlern gef√ºhrt hat. Daher wurden die Custom Instructions f√ºr chatGPT (Schritt 6) abge√§ndert, sodass nur noch ein Output in Form einer Markdown-Tabelle verlangt wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c385e34-694d-444f-b0cd-abcb38a7982c",
   "metadata": {},
   "source": [
    "## Umsetzung der Schritte 10 und 11\n",
    "\n",
    "### 10. Mehrmalige Wiederholung von Schritt 6 bis 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f0153-9d0b-409d-8412-bacc7215625e",
   "metadata": {},
   "source": [
    "### 11. Analyse der Reliabilit√§t (Berechnen von √úbereinstimmungsma√üen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce32211-8306-4dcd-a2f5-1dfa8d11723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndx = None\n",
    "ndxs = []\n",
    "\n",
    "def get_csv_filenames():\n",
    "  cwd = os.getcwd()\n",
    "  cwd = cwd + \"/User\"\n",
    "  file_names = os.listdir(cwd)\n",
    "  csv_files = []\n",
    "  for file_name in file_names:\n",
    "    if file_name.endswith('.csv'):\n",
    "      csv_path = os.path.join(cwd, file_name)\n",
    "      csv_files.append(csv_path)\n",
    "  return csv_files\n",
    "\n",
    "def interrater_reliability(file_path):\n",
    "  df = pd.read_csv(file_path, usecols=range(5))\n",
    "  # √úbereinstimmung:\n",
    "  # 0|1 = perfekte √úbereinstimmung\n",
    "  # 0.5 = perfekter Zufall\n",
    "  ndx = df.mean(axis=1).mean(axis=0)\n",
    "  # Nach der Transformation:\n",
    "  # 1 = perfekte √úbereinstimmung\n",
    "  # 0 = perfekter Zufall\n",
    "  ndx = round(abs((ndx - 0.5)) * 2, 3)\n",
    "  return(ndx)\n",
    "\n",
    "csv_files = get_csv_filenames()\n",
    "for csv_file in csv_files:\n",
    "  ndx = interrater_reliability(csv_file)\n",
    "  ndxs.append(ndx)\n",
    "ndxtot = round(sum(ndxs) / len(ndxs), 3)\n",
    "\n",
    "filename = os.getcwd() + \"/User/ubereinstimmung.txt\"\n",
    "with open(filename, 'w') as file:\n",
    "  file.write(\";\".join(map(str, ndxs)) + \"\\n\")\n",
    "  file.write(str(ndxtot) + \"\\n\")\n",
    "\n",
    "if (demo):\n",
    "  csv_files = get_csv_filenames()\n",
    "  print(csv_files)\n",
    "  ndx = interrater_reliability(csv_files[0])\n",
    "  #print(df)\n",
    "  print(\"√úbereinstimmungsindex: \" + str(ndx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c56677-1248-48e8-878b-7211e98f3c01",
   "metadata": {},
   "source": [
    "## To-do\n",
    "\n",
    "### 12. Analyse der Validit√§t anhand von menschlichen Kodierer:innen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d75015-55fa-4c9b-a06e-5735ee3dcbb0",
   "metadata": {},
   "source": [
    "## Abschlussbemerkungen\n",
    "Bei den vorliegenden Ausgangsdaten handelt es sich um Protokolle von geschriebenen oder gesprochenen Unterhaltungen von Studierenden der Universit√§t Graz. Damit handelt es sich um pers√∂nliche Daten, und es muss sichergestellt werden, dass diese ordnungsgem√§√ü gehandhabt und gesch√ºtzt werden, um die Privatsph√§re der Betroffenen zu wahren. Das beinhaltet die Pseudonymisierung von personenbezogenen Daten, bevor sie in die Analyse eingehen, sowie strenge Zugriffskontrollen, um sicherzustellen, dass nur autorisierte Personen Zugriff auf die Daten haben.\n",
    "\n",
    "Des Weiteren ist die Transparenz der Datenverarbeitung ein wichtiger Punkt, damit die Proband:innen √ºberhaupt in der Lage sind, eine informierte Zustimmung zur Verarbeitung ihrer Daten zu geben.\n",
    "\n",
    "Dar√ºber hinaus werden geringf√ºgige √Ñnderungen an den LLMs oft ohne Wissen der Benutzer:innen durchgef√ºhrt, oder ohne dass Nutzer:innen darauf Einfluss haben (z.B. Bugfixes, Ver√§nderungen der Gr√∂√üe des Kontext-Fensters oder der maximalen Anzahl von Nachrichten pro Zeiteinheit bei starker Auslastung). Bereits geringf√ºgige √Ñnderungen k√∂nnen jedoch in komplexen Systemen gravierende Auswirkungen nach sich ziehen. So k√∂nnte es auch auf diese Art zu √Ñnderungen in den Kodierungen kommen.\n",
    "\n",
    "Weiters sollte jedes KI-generierte Output kritisch betrachtet werden, da LLMs daf√ºr bekannt sind, dass sie halluzinieren (z.B. Tonmoy et al., 2024).\n",
    "\n",
    "Aus all diesen Gr√ºnden m√ºssen die Ergebnisse der KI-gest√ºtzen Analyse regelm√§√üig evaluiert werden, um eventuelle Beeinflussungen von Forschungsergebnissen fr√ºhzeitig zu erkennen und einer Kompromittierung der wissenschaftlichen Integrit√§t vorzubeugen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5ed1c-ba6e-4d86-aea4-62498653bd7d",
   "metadata": {},
   "source": [
    "# Literatur\n",
    "* Bortz, J., & D√∂ring, N. (2006). *Forschungsmethoden und Evaluation f√ºr Human- und Sozialwissenschaftler.* Springer.\n",
    "* Brand, St. (2023). Meet TRACI ‚Äì *User‚Äôs Guide to the TRACI Prompt Framework for ChatGPT.* Blur Factor New Media. https://structuredprompt.com/free-traci-users-guide-white-paper/\n",
    "* Mayring (2014). *Qualitative Content Analysis. Theoretical Foundation, Basic Procedures and Software Solution.* Author. http://nbn-resolving.de/urn:nbn:de:0168-ssoar-395173\n",
    "* Gunawardena, C. (1995). Social presence theory and implications for interaction collaborative learning in computer conferences. *International Journal of Educational Telecommunications, 1,* 147-166. Retrieved April 28, 2024 from https://www.learntechlib.org/primary/p/15156/\n",
    "* Mollick, E. (2023, November 01). *Working with AI: Two paths to prompting.* One Useful Thing. https://www.oneusefulthing.org/p/working-with-ai-two-paths-to-prompting\n",
    "* Morgan, D. (2023). Exploring the Use of Artificial Intelligence for Qualitative Data Analysis: The Case of ChatGPT. *International Journal of Qualitative Methods, 22,* 1-10. https://doi.org/10.1177/16094069231211248\n",
    "* Tonmoy, T., Zaman, M., Jain, V., Rani, A., Rawte, V., Chadha, A., Das, A. (2024). *A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models.* arXiv. https://doi.org/10.48550/arXiv.2401.01313\n",
    "* T√∂rnberg, P. (2023). *How to use Large Language Models for Text Analysis.* arXiv. https://doi.org/10.48550/arXiv.2307.13106\n",
    "* Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Qu., & Zhou, D. (2023), *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.* arXiv. https://doi.org/10.48550/arXiv.2201.11903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad517a6d-afcc-40b7-97c4-31b7dd1f8974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
