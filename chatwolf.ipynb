{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f288f3a7-35f7-4d85-a402-7b61cb274fc5",
   "metadata": {},
   "source": [
    "# Chatwolf\n",
    "\n",
    "Das Projekt \"Chatwolf\" versucht die Möglichkeiten von Services, die generative künstliche Intelligenz (KI) einer breiten Öffentlichkeit zur Verfügung stellt, in den Prozess der quantitativen Inhaltsanalyse einzubinden, und niederschwellig und kostengünstig nutzbar zu machen.\n",
    "\n",
    "![Chatwolf Bild](./chatwolf_round_small.png)\n",
    "\n",
    "1. [Hintergrund](#Hintergrund)\n",
    "2. [Workflow](#Workflow)\n",
    "3. [Umsetzung I](#Umsetzung-der-Schritte-1-bis-5)\n",
    "4. [Umsetzung II](#Umsetzung-der-Schritte-7-bis-9)\n",
    "5. [Umsetzung III](#Umsetzung-der-Schritte-10-und-11)\n",
    "6. [Abschlussbemerkungen](#Abschlussbemerkungen)\n",
    "7. [Literatur](#Literatur)\n",
    "\n",
    "\n",
    "## Hintergrund\n",
    "In einem laufenden Projekt im Arbeitsbereich Pädagogische Psychologie des Instituts für Psychologie der Universität Graz, im Rahmen dessen mehrere Masterarbeiten betreut werden, werden verschiedene Aspekte von Interaktionen zwischen Tutor:innen und Lernenden betrachtet. Dabei werden drei Formen der Interaktion miteinander verglichen: 1) maschinelle:r Tutor:in schreibt (chatGPT-4 mit angepassten Instruktionen), 2) menschliche:r Tutor:in schreibt, 3) menschliche:r Tutor:in spricht. Bei den Lernenden handelt es sich dabei immer um Proband:innen. Mit den Mitteln quantitativer Inhaltsanalyse wird u.a. der Grad der sozialen Präsenz zwischen diesen verschiedenen Bedingungen verglichen. Unter sozialer Präsenz versteht man das Ausmaß, in dem ein:e Kommunikationspartner:in in der medienvermittelten Kommunikation als reale Person wahrgenommen wird (Gunawardena, 1995).\n",
    "\n",
    "Inhaltsanalyse größerer Datenmengen ist aufwändig und nimmt viel Zeit in Anspruch. Zur Unterstützung existieren verschiedene Softwarelösungen, die sich in Grad der Unterstützung, Bedienungskomfort und v.a. Kosten stark unterscheiden. Generative KI, im speziellen Large Language Modelle (LLMs), eignen sich gut für die Verarbeitung und Zusammenfassung von Text (Törneberg, 2023). Der Einsatz von generativer KI für die Inhaltsanalyse im Bereich psychologischer und soziologischer Forschung wurde auch bereits in kleinem Rahmen getestet und dokumentiert (z.B. Morgan, 2023). Erste eigene Erfahrungen haben gezeigt, dass die Entwicklung von Instruktionen für die Inhaltsanalyse mit chatGPT-4 zwar schnell scheinbar brauchbare Ergebnisse liefert, diese aber nicht zuverlässig reproduzierbar sind. Insbesondere wird Textmaterial bei mehrmaligem Durchlauf unter Verwendung identischer Instruktionen in unterschiedliche Kodiereinheiten zerlegt. Es ist zwar davon auszugehen, dass sich das mit kommenden Versionen der entsprechenden LLMs verbessern wird. Im Moment ist diese Herangehensweise jedoch nicht für einen großteils automatischen Ablauf geeignet.\n",
    "\n",
    "Daher wurde ein Workflow entwickelt, mit dem überprüft werden soll, in wie weit aktuelle LLMs zur Unterstützung der Inhaltsanalyse eingesetzt werden können.\n",
    "\n",
    "## Workflow\n",
    "1. Input: Chat-Protokolle (DOCX)\n",
    "2. Konvertierung ins Markdown-Format\n",
    "3. Aufteilen in Anteil Tutor:in und Lernende:r\n",
    "4. Syntax-Analyse: Aufteilen in Kodiereinheiten\n",
    "5. Output: Kodiereinheiten (Markdown-Datei)\n",
    "6. Manuelle Überprüfung der Output-Dateien\n",
    "7. Input: Übergabe der Kodiereinheiten an chatGPT\n",
    "8. Kodierung durch chatGPT\n",
    "9. Output: Ergebnisse der Kodierung (CSV)\n",
    "10. Mehrmalige Wiederholung der Kodierung (7 bis 9)\n",
    "11. Analyse der Reliabilität (Übereinstimmungsmaße)\n",
    "12. Analyse der Validität anhand von menschlichen Kodierer:innen\n",
    "\n",
    "Die Schritte 1 bis 5 werden in Form eines Python-Programmes umgesetzt. Bei dem verwendeten Textmaterial handelt es sich um Chat-Protokolle von Dialogen zwischen Tutor:innen und Lernenden. Es kommen u.a. Python-Module zum Konvertieren zwischen Dateiformaten (Schritt 2) und zum Verarbeiten natürlicher Sprache (Schritt 4) zum Einsatz. Die dafür verwendeten Module sind `pypandoc`und `spaCy`.\n",
    "\n",
    "In Schritt 6 erfolgt die manuelle Überprüfung und evtl. Anpassung der zuvor generierten Dateien.\n",
    "\n",
    "Die Schritte 7 bis 9 finden mit Unterstützung von chatGPT statt. In Schritt 7 wird die Liste der Kodiereinheiten (Tutor:in und Lernende:r getrennt) in Form einer zuvor generierten strukturierten Textdatei im Markdown-Format an chatGPT übergeben. In Schritt 8 werden die Kodiereinheiten im Hinblick auf soziale Präsenz beurteilt. Dazu wird ein Kodierschema für menschliche Kodierer:innen entwickelt, auf dessen Basis wiederum detaillierte Instruktionen für chatGPT entwickelt werden. Die Instruktionen für chatGPT folgen dabei den Prinzipien von Structured Prompting (Mollick, 2023; Brand, 2023) und Chain-of-Thought Prompting (Wei et al., 2023). In Schritt 9 werden die  Ergebnisse der Kodierung in Form einer Textdatei im CSV-Format gesichert.\n",
    "\n",
    "Anschließend (Schritt 10) werden die Schritte 7 bis 9 mehrmals wiederholt. Dabei ist es wichtig, dass diese Wiederholungen jeweils in einer neuen Instanz von chatGPT stattfinden, damit vorangegangene Eingaben und Ergebnisse, die sich noch im Kontext-Fenster befinden, die neuen Ergebnisse nicht kontaminieren.\n",
    "\n",
    "Schritt 11 wird in Form eines Python-Programmes umgesetzt. Dabei werden die Ergebnisse der mehrfach durchgeführten Kodierungsprozesse an den selben Ausgangsdaten auf Übereinstimmung geprüft. Da hier in jedem Datensatz alle Kodiereinheiten kodiert werden, kann ein Signifikanztest durchgeführt werden, der auf eine Abweichung von Null (= zufällige Übereinstimmung) prüft. Damit kann eine statistische Aussage getroffen werden, wie zuverlässig die wiederholten Kodierungen von chatGPT unter Anwendung der entwickelten Instruktionen sind.\n",
    "\n",
    "In Schritt 12 wird die Qualität der für Schritt 8 entwickelten Instruktionen evaluiert, indem das Ergebnis der Kodierung durch chatGPT unter Anwendung der entwickelten Instruktionen mit dem Ergebnis der Kodierung durch menschliche Kodierer:innen unter Anwendung des entwickelten Kodierungsschemas verglichen wird (Interraterreliabilität; Bortz & Döring, 2006). Dafür kann z.B. das Python-Modul `scipy.stats.kendalltau` eingesetzt werden. Bei bedeutsamen Abweichungen müssen Kodieranweisungen und Instruktionen überarbeitet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88297c-a5c2-41f8-92cb-cb60e73cc132",
   "metadata": {},
   "source": [
    "## Umsetzung der Schritte 1 bis 5<a name=\"umsetzung_i\" />\n",
    "### 0. Vorbereitungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af527c-015f-442c-bb55-452357ee27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies installieren\n",
    "#!pip install pypandoc spacy pyperclip pingouin\n",
    "#!spacy download de_core_news_md\n",
    "\n",
    "import os, re, pypandoc, spacy, pyperclip\n",
    "import pandas as pd\n",
    "\n",
    "demo = True    # Demo-Durchlauf\n",
    "demo_file = 0  # Welche Datei soll im Demo-Durchlauf verwendet werden?\n",
    "\n",
    "# Soll spacy dependency parse oder statistical sentence segmenter verwenden?\n",
    "# Dependency parse ist besser bei Texten wie Nachrichten, Zeitungsartikel etc.\n",
    "# Statistical sentence segmenter ist besser bei unklarer Punktuation.\n",
    "# nlp.enable_pipe wird beim import von spacy auf \"None\" gesetzt.\n",
    "segmenter = True\n",
    "if segmenter: \n",
    "  nlp = spacy.load('de_core_news_md', exclude=[\"parser\"])\n",
    "  nlp.enable_pipe(\"senter\")\n",
    "else:\n",
    "  nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e524fc-213f-4592-aa4f-daa2e418b2b2",
   "metadata": {},
   "source": [
    "### 1. Input: Dateien im DOCX-Format (Chat-Protokolle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a011e-cadf-4af8-8151-f37e0ded6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docx_filenames():\n",
    "  cwd = os.getcwd()\n",
    "  file_names = os.listdir(cwd)\n",
    "  docx_files = []\n",
    "  for file_name in file_names:\n",
    "    if file_name.endswith('.docx'):\n",
    "      docx_path = os.path.join(cwd, file_name)\n",
    "      docx_files.append(docx_path)\n",
    "  return docx_files\n",
    "\n",
    "if (demo):\n",
    "  docx_files = get_docx_filenames()\n",
    "  print(docx_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12277464-c713-43ab-9dfe-9c276b99977d",
   "metadata": {},
   "source": [
    "### 2. Konvertierung ins Markdown-Format (und Textbereinigung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07863218-6230-4b1e-bbd2-e593d468afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_docx_to_md(docx_path):\n",
    "  md_path = re.sub(r'\\.docx$', '.md', docx_path)\n",
    "  md_text = pypandoc.convert_file(docx_path, 'md', extra_args=['--wrap=none'])\n",
    "  # Entfernen der User-Icons\n",
    "  md_text = re.sub(r'\\!\\[User\\]\\(media.+', '', md_text)\n",
    "  # Bezeichnungen für User:in und Tutor:in anpassen\n",
    "  # Unter Windows wird das Zeilenende mit $ nicht korrekt erkannt,\n",
    "  # daher muss $ unter Windows in den folgenden RegEx weg gelassen werden.\n",
    "  #md_text = re.sub(r'^\\*\\*Sie\\*\\*$', '# User', md_text, flags=re.MULTILINE)\n",
    "  #md_text = re.sub(r'^\\*\\*You\\*\\*$', '# User', md_text, flags=re.MULTILINE)\n",
    "  #md_text = re.sub(r'^\\*\\*LernenGPT\\*\\*$', '# Tutor', md_text, flags=re.MULTILINE)\n",
    "  #md_text = re.sub(r'^\\\\\\[\\d\\d\\:\\d\\d\\\\\\] Testung Paedpsy$', '# User', md_text, flags=re.MULTILINE)\n",
    "  #md_text = re.sub(r'^\\\\\\[\\d\\d\\:\\d\\d\\\\\\]\\s\\w+.*,\\s\\w+(.\\(Extern\\))?$', '# Tutor', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\*\\*Sie\\*\\*', '# User', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\*\\*You\\*\\*', '# User', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\*\\*LernenGPT\\*\\*', '# Tutor', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\\\\\[\\d\\d\\:\\d\\d\\\\\\] Testung Paedpsy', '# User', md_text, flags=re.MULTILINE)\n",
    "  md_text = re.sub(r'^\\\\\\[\\d\\d\\:\\d\\d\\\\\\]\\s\\w+.*,\\s\\w+(.\\(Extern\\))?', '# Tutor', md_text, flags=re.MULTILINE)\n",
    "  # Entfernen von Emote-Grafiken und anderen Grafiken: ![😄](media/image7.png){width=\"0...in\" height=\"0...in\"}\n",
    "  md_text = re.sub(r'!\\[(.)\\]\\(media.+png\\)\\{width.+height.+in\\\"\\}', r'\\1', md_text)\n",
    "  md_text = re.sub(r'!\\[\\]\\(media.+[(png)(jpeg)(wmf)]\\)(\\{width.+height.+in\\\"\\})?', '', md_text)\n",
    "  # Ersetzen geschützter Leerzeichen: \\xa0\n",
    "  md_text = md_text.replace(u'\\xa0', u' ')\n",
    "  # Gendern mit : macht evtl Probleme bei Erkennung von Satzgrenzen.\n",
    "  #md_text = re.sub(r'(\\w):in', r'\\1in', md_text)\n",
    "  return md_text\n",
    "\n",
    "if (demo):\n",
    "  md_text = convert_docx_to_md(docx_files[demo_file])\n",
    "  print(md_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc711fb-bf58-4cd9-be54-25fa138f1616",
   "metadata": {},
   "source": [
    "### 3. Aufteilen der Chat-Protokolle in Tutor:in und Lernende:r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d7071-73e9-4891-afe5-08b09a3d1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_speakers(md_text):\n",
    "  tmp = []\n",
    "  first_speaker = \"\"\n",
    "  current_speaker = \"\"\n",
    "  speakers = {\n",
    "    \"User\": [],\n",
    "    \"Tutor\": []\n",
    "  }\n",
    "\n",
    "  # Erste:n Sprecher:in festhalten\n",
    "  for line in md_text.splitlines():\n",
    "    if line == \"# User\":\n",
    "      first_speaker = \"User\"\n",
    "      break\n",
    "    elif line == \"# Tutor\":\n",
    "      first_speaker = \"Tutor\"\n",
    "      break\n",
    "  # Aufteilen der Datei in Anteile von User und Tutor\n",
    "  for line in md_text.splitlines():\n",
    "    if line == \"# User\":\n",
    "      current_speaker = \"User\"\n",
    "    elif line == \"# Tutor\":\n",
    "      current_speaker = \"Tutor\"\n",
    "    elif (line and current_speaker in speakers):\n",
    "      speakers[current_speaker].append(line)\n",
    "  return speakers, first_speaker\n",
    "\n",
    "if (demo):\n",
    "  speakers, first_speaker = split_speakers(md_text)\n",
    "  print(first_speaker)\n",
    "  print(speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48522c2b-faa7-46ae-91a3-74992b0b73fb",
   "metadata": {},
   "source": [
    "### 4. Syntax-Analyse: Aufteilen in Kodiereinheiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130dfdc5-109b-4db3-ad7d-fe6015530e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_props(speakers):\n",
    "  props = {\n",
    "    \"User\": [],\n",
    "    \"Tutor\": []\n",
    "  }\n",
    "  for actor in [\"User\", \"Tutor\"]:\n",
    "    for x in speakers[actor]:\n",
    "      doc = nlp(x)\n",
    "      for sentence in doc.sents:\n",
    "        props[actor].append(sentence.text.strip())\n",
    "  return props\n",
    "\n",
    "if (demo):\n",
    "  props = split_props(speakers)\n",
    "  print(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01dcf5-838f-4956-9b96-878160d74d3b",
   "metadata": {},
   "source": [
    "### 5. Output: Textdatei (Markdown) mit Kodiereinheiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d79ef-f5a5-4672-b1d2-bfc6c6bd8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text(md_text, props, path):\n",
    "  path = os.path.splitext(path)[0]\n",
    "  # Gesamte Konversation speichern\n",
    "  md_path = path + '.md'\n",
    "  with open(md_path, 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(md_text)\n",
    "  # User- und Tutor-Seite der Konversation speichern\n",
    "  for actor in [\"User\", \"Tutor\"]:\n",
    "    props_path = path + '_' + actor + '.md'\n",
    "    with open(props_path, 'w', encoding=\"utf-8\") as file:\n",
    "      for x in props[actor]:\n",
    "        file.write(x)\n",
    "        file.write(\"\\n\")\n",
    "  print(f'Processed {path}.docx')\n",
    "\n",
    "if demo:\n",
    "  save_text(md_text, props, docx_files[demo_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954e2cf-3482-4fd2-be20-8adaf2e062a1",
   "metadata": {},
   "source": [
    "### Optional: Gesamtdurchlauf der Schritte 1 bis 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7af442-77a4-47cf-9d37-b2b6d3a46432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesamtdurchlauf der Schritte 1 bis 5\n",
    "docx_files = get_docx_filenames()\n",
    "for file_name in docx_files:\n",
    "  md_text = convert_docx_to_md(file_name)\n",
    "  speakers, first_speaker = split_speakers(md_text)\n",
    "  props = split_props(speakers)\n",
    "  save_text(md_text, props, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f890b2-e330-4109-878b-0282e897c2d6",
   "metadata": {},
   "source": [
    "### Optional: Verzeichnis aufräumen\n",
    "**CAVE: Alle Dateien im aktuellen Verzeichnis mit der Endung `.md` werden gelöscht!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a42ee-6e02-415b-bbba-1e6e246aa0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnis aufräumen\n",
    "# ACHTUNG: Alle Dateien mit Endung .md werden gelöscht!\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "file_names = os.listdir(cwd)\n",
    "for file_name in file_names:\n",
    "  if file_name.endswith('.md'):\n",
    "    os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6004b3-f1eb-41fc-bf78-2cbd6146c7ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Manuelle Überprüfung der Output-Dateien\n",
    "Die generierten Output-Dateien müssen manuell überprüft und evtl. korrigiert werden. Die deutschsprachigen Trainingsdaten für das verwendete Modul `spacy` wurden v.a. anhand von deutschspachigen Zeitungsartikeln gewonnen. Die Chats entsprechen aber bei weitem nicht dieser Qualität, weshalb manuell nachgebessert werden muss.\n",
    "\n",
    "Um ein Überschreiben der korrigierten Dateien zu vermeiden, wird empfohlen, diese unter neuem Namen oder in einem neuen Verzeichnis zu sichern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17b483-2baf-4f46-a4d1-d703f85dfaec",
   "metadata": {},
   "source": [
    "## Umsetzung der Schritte 7 bis 9\n",
    "*Die Schritte 7 bis 9 werden in ChatGPT durchgeführt.*\n",
    "\n",
    "### 7. Input: Übergabe der Liste von Kodiereinheiten an chatGPT\n",
    "Dazu wurde ein Custom GPT erstellt, das unter dem folgenden Link zu erreichen ist: **[Custom GPT: Content-Analysis](https://chatgpt.com/g/g-tURb8JB2t-content-analysis)**\n",
    "\n",
    "Die Instruktionen für dieses Custom GPT sind im Anschluss angeführt und können auch als Prompt ausgeführt werden.\n",
    "\n",
    "```\n",
    "# Custom GPT - Instructions\n",
    "\n",
    "## Task\n",
    "Process the uploaded markdown file and evaluate each line for social presence, then present the results in a table.\n",
    "\n",
    "## Role: Text Analyst\n",
    "- Analyze text based on social presence indicators.\n",
    "- Evaluate the emotional and interpersonal engagement in the text.\n",
    "- Apply consistent scoring throughout the analysis.\n",
    "\n",
    "## Audience: Scientist\n",
    "- Wants to assess social presence in a document.\n",
    "- Prefers clear and structured output.\n",
    "- Expects accurate and reliable analysis.\n",
    "- Their research and career depend on the results!\n",
    "\n",
    "## Create\n",
    "A table with two columns: 1) Social Presence Rating and 2) Unit of analysis (as per input).\n",
    "\n",
    "### Create Rules\n",
    "- Take a deep breath before you begin!\n",
    "- Think step by step!\n",
    "- Rate each line of the input material either 0 (low social presence) or 1 (high social presence).\n",
    "- Create a markdown table with two columns: \"Rating\" and \"Unit\".\n",
    "\n",
    "### Create Example\n",
    "  | Rating | Unit |\n",
    "  | --- | --- |\n",
    "  | 0 | Vielleicht mit Beistrich? |\n",
    "  | 1 | Danke dir |\n",
    "  | 0 | zeig es mir! | 0 |\n",
    "  | 1 | 😄 |\n",
    "  | 1 | like 1 |\n",
    "\n",
    "## Intent\n",
    "Help the user assess social presence in the text by providing unit-by-unit ratings.\n",
    "\n",
    "### Intent Rules\n",
    "- Focus on interpersonal connection and emotional tone.\n",
    "- Deliver the result in an easy-to-use format.\n",
    "- Make sure the results are reliable and can be reproduced.\n",
    "\n",
    "## Material\n",
    "- Textfile in markdown format.\n",
    "- Each line of the input file represents a single unit of analysis.\n",
    "```\n",
    "\n",
    "In ChatGPT kann nun eine der neu erstellten Dateien (XXX_User.md) hochgeladen werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ad096-fccb-4261-a83f-eb14812dcf88",
   "metadata": {},
   "source": [
    "### 8. Kodierung in Bezug auf soziale Präsenz\n",
    "\n",
    "Für die Kodierung kann das folgende Prompt ausgeführt werden:\n",
    "\n",
    "`Rate the social presence for each unit of analysis in the uploaded file according to your instructions.`\n",
    "\n",
    "Der Text wird beim Ausführen des nächsten Code-Blocks in die Zwischenablage kopiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6efbd-6acd-438d-a786-3e1d371dcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyperclip.copy(\"Rate the social presence for each unit of analysis in the uploaded file according to your instructions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29482dff-2da9-4b48-ae8d-49fcf1163c5d",
   "metadata": {},
   "source": [
    "### 9. Output: Textdatei (CSV) mit den Resultaten der Kodierung\n",
    "Die Ergebnisse werden von ChatGPT in Form einer CSV-Datei zum Download bereit gestellt. Eventuell muss die Erstellung der CSV-Datei explizit angefordert werden.\n",
    "\n",
    "Allerdings hat chatGPT immer wieder versucht, die CSV-Dateien auf unterschiedliche Art und Weise zu erstellen. Manchmal mit Erfolg, öfter leider ohne, da benötigte Python-Module nicht installiert waren, oder das generierte Skript zu Fehlern geführt hat. Daher wurden die Custom Instructions für chatGPT (Schritt 6) abgeändert, sodass nur noch ein Output in Form einer Markdown-Tabelle verlangt wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c385e34-694d-444f-b0cd-abcb38a7982c",
   "metadata": {},
   "source": [
    "## Umsetzung der Schritte 10 und 11\n",
    "\n",
    "### 10. Mehrmalige Wiederholung von Schritt 6 bis 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f0153-9d0b-409d-8412-bacc7215625e",
   "metadata": {},
   "source": [
    "### 11. Analyse der Reliabilität (Berechnen von Übereinstimmungsmaßen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce32211-8306-4dcd-a2f5-1dfa8d11723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndx = None\n",
    "ndxs = []\n",
    "\n",
    "def get_csv_filenames():\n",
    "  cwd = os.getcwd()\n",
    "  cwd = cwd + \"/User\"\n",
    "  file_names = os.listdir(cwd)\n",
    "  csv_files = []\n",
    "  for file_name in file_names:\n",
    "    if file_name.endswith('.csv'):\n",
    "      csv_path = os.path.join(cwd, file_name)\n",
    "      csv_files.append(csv_path)\n",
    "  return csv_files\n",
    "\n",
    "def interrater_reliability(file_path):\n",
    "  df = pd.read_csv(file_path, usecols=range(5))\n",
    "  # Übereinstimmung:\n",
    "  # 0|1 = perfekte Übereinstimmung\n",
    "  # 0.5 = perfekter Zufall\n",
    "  ndx = df.mean(axis=1).mean(axis=0)\n",
    "  # Nach der Transformation:\n",
    "  # 1 = perfekte Übereinstimmung\n",
    "  # 0 = perfekter Zufall\n",
    "  ndx = round(abs((ndx - 0.5)) * 2, 3)\n",
    "  return(ndx)\n",
    "\n",
    "csv_files = get_csv_filenames()\n",
    "for csv_file in csv_files:\n",
    "  ndx = interrater_reliability(csv_file)\n",
    "  ndxs.append(ndx)\n",
    "ndxtot = round(sum(ndxs) / len(ndxs), 3)\n",
    "\n",
    "filename = os.getcwd() + \"/User/ubereinstimmung.txt\"\n",
    "with open(filename, 'w') as file:\n",
    "  file.write(\";\".join(map(str, ndxs)) + \"\\n\")\n",
    "  file.write(str(ndxtot) + \"\\n\")\n",
    "\n",
    "if (demo):\n",
    "  csv_files = get_csv_filenames()\n",
    "  print(csv_files)\n",
    "  ndx = interrater_reliability(csv_files[0])\n",
    "  #print(df)\n",
    "  print(\"Übereinstimmungsindex: \" + str(ndx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c56677-1248-48e8-878b-7211e98f3c01",
   "metadata": {},
   "source": [
    "## To-do\n",
    "\n",
    "### 12. Analyse der Validität anhand von menschlichen Kodierer:innen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d75015-55fa-4c9b-a06e-5735ee3dcbb0",
   "metadata": {},
   "source": [
    "## Abschlussbemerkungen\n",
    "Bei den vorliegenden Ausgangsdaten handelt es sich um Protokolle von geschriebenen oder gesprochenen Unterhaltungen von Studierenden der Universität Graz. Damit handelt es sich um persönliche Daten, und es muss sichergestellt werden, dass diese ordnungsgemäß gehandhabt und geschützt werden, um die Privatsphäre der Betroffenen zu wahren. Das beinhaltet die Pseudonymisierung von personenbezogenen Daten, bevor sie in die Analyse eingehen, sowie strenge Zugriffskontrollen, um sicherzustellen, dass nur autorisierte Personen Zugriff auf die Daten haben.\n",
    "\n",
    "Des Weiteren ist die Transparenz der Datenverarbeitung ein wichtiger Punkt, damit die Proband:innen überhaupt in der Lage sind, eine informierte Zustimmung zur Verarbeitung ihrer Daten zu geben.\n",
    "\n",
    "Darüber hinaus werden geringfügige Änderungen an den LLMs oft ohne Wissen der Benutzer:innen durchgeführt, oder ohne dass Nutzer:innen darauf Einfluss haben (z.B. Bugfixes, Veränderungen der Größe des Kontext-Fensters oder der maximalen Anzahl von Nachrichten pro Zeiteinheit bei starker Auslastung). Bereits geringfügige Änderungen können jedoch in komplexen Systemen gravierende Auswirkungen nach sich ziehen. So könnte es auch auf diese Art zu Änderungen in den Kodierungen kommen.\n",
    "\n",
    "Weiters sollte jedes KI-generierte Output kritisch betrachtet werden, da LLMs dafür bekannt sind, dass sie halluzinieren (z.B. Tonmoy et al., 2024).\n",
    "\n",
    "Aus all diesen Gründen müssen die Ergebnisse der KI-gestützen Analyse regelmäßig evaluiert werden, um eventuelle Beeinflussungen von Forschungsergebnissen frühzeitig zu erkennen und einer Kompromittierung der wissenschaftlichen Integrität vorzubeugen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5ed1c-ba6e-4d86-aea4-62498653bd7d",
   "metadata": {},
   "source": [
    "# Literatur\n",
    "* Bortz, J., & Döring, N. (2006). *Forschungsmethoden und Evaluation für Human- und Sozialwissenschaftler.* Springer.\n",
    "* Brand, St. (2023). Meet TRACI – *User’s Guide to the TRACI Prompt Framework for ChatGPT.* Blur Factor New Media. https://structuredprompt.com/free-traci-users-guide-white-paper/\n",
    "* Mayring (2014). *Qualitative Content Analysis. Theoretical Foundation, Basic Procedures and Software Solution.* Author. http://nbn-resolving.de/urn:nbn:de:0168-ssoar-395173\n",
    "* Gunawardena, C. (1995). Social presence theory and implications for interaction collaborative learning in computer conferences. *International Journal of Educational Telecommunications, 1,* 147-166. Retrieved April 28, 2024 from https://www.learntechlib.org/primary/p/15156/\n",
    "* Mollick, E. (2023, November 01). *Working with AI: Two paths to prompting.* One Useful Thing. https://www.oneusefulthing.org/p/working-with-ai-two-paths-to-prompting\n",
    "* Morgan, D. (2023). Exploring the Use of Artificial Intelligence for Qualitative Data Analysis: The Case of ChatGPT. *International Journal of Qualitative Methods, 22,* 1-10. https://doi.org/10.1177/16094069231211248\n",
    "* Tonmoy, T., Zaman, M., Jain, V., Rani, A., Rawte, V., Chadha, A., Das, A. (2024). *A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models.* arXiv. https://doi.org/10.48550/arXiv.2401.01313\n",
    "* Törnberg, P. (2023). *How to use Large Language Models for Text Analysis.* arXiv. https://doi.org/10.48550/arXiv.2307.13106\n",
    "* Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Qu., & Zhou, D. (2023), *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.* arXiv. https://doi.org/10.48550/arXiv.2201.11903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad517a6d-afcc-40b7-97c4-31b7dd1f8974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
